{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063e37eb",
   "metadata": {},
   "source": [
    "# Creating Eigenfaces & Recognizing Faces from training set using cv2 functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c133bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import imageio\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf9c5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (\"Loading Trained model\")\n",
    "# # Load the trained eigenfaces model\n",
    "# model = cv2.face.EigenFaceRecognizer_create()\n",
    "# model.read('eigenfaces_model.xml')\n",
    "\n",
    "newshape = (121,160)\n",
    "eigenvecstoskip = 2\n",
    "data = np.genfromtxt('MeanAndFaces.csv', delimiter=',')\n",
    "mean = data[0]\n",
    "eigenmatrix = data[1+eigenvecstoskip:]\n",
    "data = np.genfromtxt('ACR.csv', delimiter=',')[:,eigenvecstoskip:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4ee88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "numPicsPerPerson = 8\n",
    "labels = np.zeros(numPicsPerPerson*3)\n",
    "for i in range(3):\n",
    "    labels[i*numPicsPerPerson:i*numPicsPerPerson+numPicsPerPerson] = i\n",
    "# labels = np.array([0,0,0,0,0,1,1,1,1,1,2,2,2,2,2])\n",
    "\n",
    "# Create a KNN classifier with k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Fit the classifier to the data\n",
    "knn.fit(data, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a78b712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Shape:  (480, 640, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "tile cannot extend outside image",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 102\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39m# Crop the frame\u001b[39;00m\n\u001b[0;32m    100\u001b[0m cropped_frame \u001b[39m=\u001b[39m frame[roi[\u001b[39m0\u001b[39m]:roi[\u001b[39m0\u001b[39m]\u001b[39m+\u001b[39mroi[\u001b[39m2\u001b[39m], roi[\u001b[39m1\u001b[39m]:roi[\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39mroi[\u001b[39m3\u001b[39m]]\n\u001b[1;32m--> 102\u001b[0m cropped_resized_frame \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mfromarray(cropped_frame\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39muint8\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m    104\u001b[0m grayscale_image_cropped \u001b[39m=\u001b[39m cropped_resized_frame\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mL\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[39m# Set the new size (half of original size in this example)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chazp\\anaconda3\\lib\\site-packages\\PIL\\Image.py:3103\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3101\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mtostring()\n\u001b[1;32m-> 3103\u001b[0m \u001b[39mreturn\u001b[39;00m frombuffer(mode, size, obj, \u001b[39m\"\u001b[39;49m\u001b[39mraw\u001b[39;49m\u001b[39m\"\u001b[39;49m, rawmode, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\chazp\\anaconda3\\lib\\site-packages\\PIL\\Image.py:3027\u001b[0m, in \u001b[0;36mfrombuffer\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   3024\u001b[0m         im\u001b[39m.\u001b[39mreadonly \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   3025\u001b[0m         \u001b[39mreturn\u001b[39;00m im\n\u001b[1;32m-> 3027\u001b[0m \u001b[39mreturn\u001b[39;00m frombytes(mode, size, data, decoder_name, args)\n",
      "File \u001b[1;32mc:\\Users\\chazp\\anaconda3\\lib\\site-packages\\PIL\\Image.py:2969\u001b[0m, in \u001b[0;36mfrombytes\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   2966\u001b[0m     args \u001b[39m=\u001b[39m mode\n\u001b[0;32m   2968\u001b[0m im \u001b[39m=\u001b[39m new(mode, size)\n\u001b[1;32m-> 2969\u001b[0m im\u001b[39m.\u001b[39;49mfrombytes(data, decoder_name, args)\n\u001b[0;32m   2970\u001b[0m \u001b[39mreturn\u001b[39;00m im\n",
      "File \u001b[1;32mc:\\Users\\chazp\\anaconda3\\lib\\site-packages\\PIL\\Image.py:825\u001b[0m, in \u001b[0;36mImage.frombytes\u001b[1;34m(self, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[39m# unpack data\u001b[39;00m\n\u001b[0;32m    824\u001b[0m d \u001b[39m=\u001b[39m _getdecoder(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode, decoder_name, args)\n\u001b[1;32m--> 825\u001b[0m d\u001b[39m.\u001b[39;49msetimage(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim)\n\u001b[0;32m    826\u001b[0m s \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39mdecode(data)\n\u001b[0;32m    828\u001b[0m \u001b[39mif\u001b[39;00m s[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: tile cannot extend outside image"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load video, 0 for webcam, str for path to video\n",
    "video = cv2.VideoCapture(0)\n",
    "# video = cv2.VideoCapture('test_clip.mp4')\n",
    "\n",
    "# Exit if video not opened.\n",
    "if not video.isOpened():\n",
    "    print('Could not open video!')\n",
    "    sys.exit()\n",
    "\n",
    "# Tracker Variables\n",
    "tracker = None\n",
    "roi = (0, 0, 0, 0)\n",
    "# -1 for not tracking, 0 for init tracking, 1 for update tracking\n",
    "tracking_flag = -1\n",
    "\n",
    "# Loop simulate Camera Preview Callback\n",
    "while True:\n",
    "\n",
    "    # Capture user Key Press to simulate App Control\n",
    "    key = cv2.waitKey(1) & 0xff\n",
    "    # User Press Enter\n",
    "    if key == 13:\n",
    "        # Not tracking\n",
    "        if tracking_flag == -1:\n",
    "            # Pause and let user select ROI\n",
    "#             roi = cv2.selectROI(frame, False) # Manual Setting of ROI in seperate window\n",
    "#             print(\"ROI: \", roi)\n",
    "            print(\"Frame Shape: \", frame.shape) # Frame Shape:  (480, 640, 3)\n",
    "            \n",
    "            # Init tracking\n",
    "            tracking_flag = 0\n",
    "        # Is tracking\n",
    "        if tracking_flag == 1:\n",
    "            # Reset ROI\n",
    "            roi = (0, 0, 0, 0)\n",
    "            # Clear Tracker\n",
    "            tracker.clear()\n",
    "            # Stop tracking\n",
    "            tracking_flag = -1\n",
    "    # User Press ESC\n",
    "    elif key == 27:\n",
    "        break\n",
    "    \n",
    "    # Start timer\n",
    "    start = cv2.getTickCount()\n",
    "\n",
    "    # Read Next frame.\n",
    "    read_success, frame = video.read()\n",
    "    if not read_success:\n",
    "        print('Cannot read video file!')\n",
    "        sys.exit()\n",
    "\n",
    "    if tracking_flag == -1:\n",
    "\n",
    "        # Display Text\n",
    "        cv2.putText(frame, \"Press ENTER to select ROI!\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "        \n",
    "        # Get frame dimensions\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        w = 178 # The width of the selected ROI.\n",
    "        h = 218 # The height of the selected ROI.            \n",
    "        x = int(width/2 - w/2) # The x-coordinate of the top-left corner of the selected region of interest (ROI).\n",
    "        y = int(height/2 - h/2) # The y-coordinate of the top-left corner of the selected ROI.\n",
    "\n",
    "        # (218 # of rows, 178 # of columns)\n",
    "\n",
    "        roi = (x, y, w, h)\n",
    "\n",
    "        # Draw ROI Rectangle\n",
    "        p1 = (int(roi[0]), int(roi[1]))\n",
    "        p2 = (int(roi[0] + roi[2]), int(roi[1] + roi[3]))\n",
    "        cv2.rectangle(frame, p1, p2, (255, 0, 0), 2, 1)\n",
    "\n",
    "\n",
    "    elif tracking_flag == 0:\n",
    "\n",
    "        # Initialize KCF Tracker and Start Tracking\n",
    "        # 1. Create a KCF Tracker\n",
    "        # 2. Initialize KCF Tracker with grayscale image and ROI\n",
    "        # 3. Modify tracking flag to start tracking\n",
    "        # Your code starts here\n",
    "        #cv2.getTickFrequency()\n",
    "        # print(\"ROI: \", roi) # ROI:  (240, 320, 178, 218)\n",
    "        tracker = cv2.TrackerKCF_create()\n",
    "        status = tracker.init(frame,roi)\n",
    "        \n",
    "        tracking_flag = 1\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Update tracking result is succeed\n",
    "        # If failed, print text \"Tracking failure occurred!\" at top left corner of the frame\n",
    "        # Calculate and display \"FPS@fps_value\" at top right corner of the frame\n",
    "        # Your code starts here\n",
    "        read_success, roi = tracker.update(frame)\n",
    "        \n",
    "        # Crop the frame\n",
    "        cropped_frame = frame[roi[0]:roi[0]+roi[2], roi[1]:roi[1]+roi[3]]\n",
    "\n",
    "        cropped_resized_frame = Image.fromarray(cropped_frame.astype('uint8'))\n",
    "\n",
    "        grayscale_image_cropped = cropped_resized_frame.convert('L')\n",
    "\n",
    "        # Set the new size (half of original size in this example)\n",
    "        new_size = newshape #defined above!!!\n",
    "\n",
    "        # Resize the image using the Lanczos filter (you can use other filters too)\n",
    "        resized_cropped_image = grayscale_image_cropped.resize(new_size, resample=Image.LANCZOS)\n",
    "\n",
    "        resize_crop_img = np.array(resized_cropped_image)\n",
    "\n",
    "        if read_success == False:\n",
    "            cv2.putText(frame, \"Tracking failure occured\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "        else:\n",
    "            # Load the image to be recognized\n",
    "            # image = cv2.imread(frame, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # print (\"Resizing Frame\")\n",
    "            # Resize the image to the same size as the training images\n",
    "            # image = cv2.resize(frame, (720, 1280))\n",
    "            \n",
    "            # Convert the image to grayscale\n",
    "#             gray_frame = cv2.cvtColor(resized_cropped_image, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            flattened_gray = np.ndarray.flatten(resize_crop_img) - mean            \n",
    "            \n",
    "            \n",
    "            finaldata = eigenmatrix @ flattened_gray\n",
    "            \n",
    "            \n",
    "            \n",
    "            # print (\"Computing Eigenface confidence\")\n",
    "            # Compute the eigenface coefficients for the image\n",
    "            # The predict function returns the predicted label and the confidence score\n",
    "#             label, confidence = model.predict(cropped_frame)\n",
    "            \n",
    "            \n",
    "\n",
    "        fps_value = int(cv2.getTickFrequency()/(cv2.getTickCount() - start))\n",
    "        cv2.putText(frame, \"FPS@\"+str(fps_value), (500, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "#         cv2.putText(frame, \"Predicted Label:\"+str(label), (100, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "#         cv2.putText(frame, \"Confidence Score:\"+str(confidence), (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "        \n",
    "\n",
    "        label = knn.predict(finaldata.reshape(1,-1))\n",
    "    \n",
    "        if (label == 0):\n",
    "            cv2.putText(frame, \"Predicted label: Aahan\", (100, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "        elif (label == 1):\n",
    "            cv2.putText(frame, \"Predicted label: Chaz\", (100, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "        elif (label == 2):\n",
    "            cv2.putText(frame, \"Predicted label: Rutvik\", (100, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "        \n",
    "#         cv2.putText(frame, \"Confidence score: {}\".format(confidence), (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Draw ROI Rectangle\n",
    "    p1 = (int(roi[0]), int(roi[1]))\n",
    "    p2 = (int(roi[0] + roi[2]), int(roi[1] + roi[3]))\n",
    "    cv2.rectangle(frame, p1, p2, (255, 0, 0), 2, 1)\n",
    "    # Display result\n",
    "    cv2.imshow(\"ECE420 Lab7\", frame)\n",
    "    \n",
    "\n",
    "# # Print the predicted label and confidence score\n",
    "# print('Predicted label: {}'.format(label))\n",
    "# print('Confidence score: {}'.format(confidence))\n",
    "# print(\"0 = Aahan; 1 = Chaz; 2 = Rutvik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f6249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
